# Importing the necessary libraries
import pandas as pd 
import numpy as np 
from sklearn.preprocessing import OneHotEncoder ,LabelEncoder

from sklearn.compose import ColumnTransformer

# Load the dataset
dataset=pd.read_csv('titanic.csv')

# Identify the categorical data

categorical_feature = ['Sex', 'Embarked', 'Pclass']

# Implement an instance of the ColumnTransformer class
ct=ColumnTransformer(transformers=[('encoder',OneHotEncoder(),categorical_feature)],remainder='passthrough')

# Apply the fit_transform method on the instance of ColumnTransformer
X=ct.fit_transform(dataset)


# Convert the output into a NumPy array
X=np.array(X)

# Use LabelEncoder to encode binary categorical data
le = LabelEncoder()
y = le.fit_transform(dataset['Survived'])

# Print the updated matrix of features and the dependent variable vector
print("Updated matrix of features: \n", X)
print("Updated dependent variable vector: \n", y)


***
# Import necessary libraries
import pandas as pd
import numpy as np

from sklearn.preprocessing import OneHotEncoder,LabelEncoder
from sklearn.preprocessing import StandardScaler

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer

from sklearn.impute import SimpleImputer
# Load the Iris dataset

dataset=pd.read_csv('iris.csv')
# Separate features and target
X=dataset.iloc[:,:-1].values
y=dataset.iloc[:,-1].values

# Split the dataset into an 80-20 training-test set

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1)

# Apply feature scaling on the training and test sets
scaler=StandardScaler()
X_train=scaler.fit_transform(X_train)
X_test=scaler.transform(X_test)

# Print the scaled training and test sets
print("Scaled Training Set:")
print(X_train)
print("\nScaled Test Set:")
print(X_test)


## Artifical Neural Network

import tensorflow as tf

# initializing the ann

ann=tf.keras.models.Sequential()

# adding the input layer

ann.add(tf.keras.layers.Dense(units='6',activation='relu'))

# adding the first hidden layer

ann.add(tf.keras.layers.Dense(units='6',activation='relu'))

# adding the  output layer

ann.add(tf.keras.layers.Dense(units='1',activation='sigmoid'))   # for dataset having more than 2 catgory activation function should be soft max


#Compiling the ANN

ann.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])  # for dataset having more than 2 catgory loss function should be loss='categorical_crossentropy'

#Training the ANN

ann.fit(X_train, y_train, batch_size=32, epochs=100)

in deep learning feature scaling is mandatory and feature scaling is applied to every columns even encoded data

The Adam optimizer, short for “Adaptive Moment Estimation,” is an iterative optimization algorithm used to minimize the loss function during the training of neural networks.

#Confusion matrix

from sklearn.metrics import confusion_matrix,accuracy_score

cm=confusion_matrix(y_test,y_pred)
print(cm)

accuracy = accuracy_score(y_,test, y_pred)
print(accuracy)


-- we should not apply feature scaling standardisation to encoded data because doing so will make actual interpretation of encoded data

-- in multiple linear regression , feature scaling is absolutely not required


The Mean Squared Error (MSE) and R-squared score are both metrics used to evaluate the performance of regression models. Here's what each of these metrics means:

Mean Squared Error (MSE):

-The Mean Squared Error is a measure of the average squared difference between the actual and predicted values in the dataset.
-It quantifies the average magnitude of the errors or residuals between the predicted values and the actual values.
-A lower MSE indicates better model performance, as it means the model's predictions are closer to the actual values.
-In your case, the MSE value of approximately 4.86 billion indicates that, on average, the squared difference between the predicted and actual target values is quite large. This suggests that the model's predictions may have significant errors.


R-squared Score:

-The R-squared (R2) score is a statistical measure that represents the proportion of the variance in the dependent variable that is explained by the independent variables in the model.
-It ranges from 0 to 1, where 1 indicates a perfect fit (the model explains all the variability of the response data around its mean) and 0 indicates that the model does not explain any of the variability.
-A higher R-squared score indicates a better fit of the model to the data.
-In your case, the R-squared score of approximately 0.64 indicates that the model explains around 64% of the variance in the target variable. While this is not perfect, it suggests that the model has some explanatory power.

R square is just the square of R , the statistically significant R was 0.9 means R square is 0.81 , the relationship between the two variables explains 81% of variation of data. R square is considered better than R as it is Easier to interpret. 

R square is percentage of variation explained by the relationship between two variables(our ml model).

--Adjusted R-square:  Adjusted R-squared is a modified version of the R-squared statistic that adjusts for the number of predictors in a regression model. Unlike the standard R-squared, which can only increase as more variables are added to the model, adjusted R-squared can decrease if the added predictors do not improve the model sufficiently. This makes it a more reliable metric for comparing models with different numbers of predictors.

R-squared:

Represents the proportion of the variance in the dependent variable that is predictable from the independent variables.
Can be interpreted as the goodness of fit of the model.
Always increases or remains the same when new predictors are added, regardless of their relevance.

Adjusted R-squared:

Adjusts the R-squared value based on the number of predictors and the sample size.
Takes into account the number of predictors, penalizing the addition of irrelevant variables.
Provides a more accurate measure of the goodness of fit when comparing models with different numbers of predictors.


Adjusted R-squared takes into account the number of predictors in the model and adjusts the R-squared value accordingly. It penalizes the addition of non-significant predictors.

Analogy
Imagine you're a teacher grading students on a project. The students are allowed to have team members (predictors) to help them with the project (predicting the outcome).

R-squared: Think of this as the total score of the project. It shows how well the project turned out.
Adjusted R-squared: This is like adjusting the project score based on how many team members helped. If more team members (predictors) are added, the expectation is that the project should be better. However, if adding more team members doesn't significantly improve the project, the adjusted score reflects that by not increasing (or even decreasing).

Why Adjusted R-squared?
When you add more predictors to a model, the R-squared value almost always increases, regardless of whether those predictors are meaningful. This can be misleading. Adjusted R-squared adjusts for the number of predictors and only increases if the new predictors improve the model more than would be expected by chance.

There are 3 types of Decision Tree.
--CART (Classification and Regression Trees): This is a general term for trees that can be used for both classification and regression problems.
CART uses the Gini index (for classification) and mean squared error (for regression) to determine splits.

--ID3 (Iterative Dichotomiser 3): One of the earliest algorithms for creating classification trees. Uses information gain (derived from entropy) to make splits.

--CHAID (Chi-squared Automatic Interaction Detection): A tree-building algorithm that uses the Chi-square test for association to make splits.
Typically used for categorical target variables and can handle multi-level splits.

---------------------------*****************************--------------------------------------
**** what is random forest?
a Random Forest is a machine learning technique used for classification and regression tasks. It works by creating a collection (or "forest") of decision trees during training time and outputting the mode of the classes (classification) or mean prediction (regression) of the individual trees.

Building Multiple Trees:

In a Random Forest, many decision trees are built using different subsets of the data. These subsets are randomly chosen.
Each tree is trained on a slightly different set of data and may make slightly different decisions.
Combining Trees' Results:

After all the trees have been built, the Random Forest combines their results.
For classification tasks, it takes a vote from each tree to decide the most common class (like majority voting).
For regression tasks, it averages the predictions from all the trees to produce a final result.

Example Analogy
Imagine you have a group of doctors diagnosing patients. Each doctor looks at different aspects and asks different questions to make their diagnosis. One doctor might ask about symptoms, another might check medical history, and another might look at test results. Each doctor makes their own decision. The final diagnosis is decided by taking the majority vote from all the doctors. This way, even if one doctor makes a mistake, the overall decision is likely to be correct because it’s based on the collective wisdom of the group.

In the same way, a Random Forest relies on the collective decision-making of multiple decision trees to make more accurate and reliable predictions.


** feature scaling is not applied to decision tree or random forest or adaboost (ensemble) because decision tree works by splitting data using node of tree and does not involve any equations like linear regression or neural network. 

** Feature scaling is generally needed in those algorithm for which there is implicit equations relating for Features and dependent variables. Like Support vector machine , support vector regressor, Logistic regression

Here’s a breakdown of popular machine learning algorithms and whether **feature scaling is required** or not, based on how the algorithm works:

---

### **Feature Scaling is Required**:
These algorithms are **sensitive to the scale** of input features, as they rely on distance-based calculations, gradient descent, or involve regularization.

1. Support Vector Machines (SVM/SVR)**:
   - Distance-based algorithm; scaling ensures that no single feature dominates.
  
2. K-Nearest Neighbors (KNN)**:
   - Distance-based algorithm; scaling ensures that features contribute equally to distance calculations.

3. Logistic Regression**:
   - Uses gradient descent for optimization and can have regularization. Scaling helps in faster convergence and fair treatment of all features.

4. Neural Networks (e.g., Multilayer Perceptron, Deep Learning)**:
   - Gradient descent-based algorithms. Scaling ensures better training, prevents vanishing gradients, and avoids neuron saturation.

5. Gradient Boosting (e.g., XGBoost, LightGBM, CatBoost)**:
   - Feature scaling can improve performance, though not strictly necessary, particularly when tree-based, but it is helpful when used with some regularization techniques.

6. K-Means Clustering**:
   - Distance-based algorithm; unscaled features can distort clusters.

7. Principal Component Analysis (PCA)**:
   - PCA projects data onto principal components based on variance, so features with larger scales will dominate unless scaled.

8. Linear Discriminant Analysis (LDA)**:
   - Similar to PCA; sensitive to variance and scales of features.

9. Polynomial Regression** (if using gradient descent):
   - Gradient-based optimization benefits from feature scaling.

10. Perceptron**:
    - A linear classifier that benefits from feature scaling for convergence.

11. Ridge and Lasso Regression** (and other regularized regression models):
    - Regularization adds penalties based on coefficients. Scaling ensures that regularization applies uniformly across features.

---

### **Feature Scaling is Not Required**:
These algorithms are generally **not sensitive to the scale** of the input features, often because they are tree-based or coefficient-based in a way that naturally adjusts for different feature scales.

1. Decision Trees (e.g., CART, C4.5)**:
   - Decision trees are not affected by the scale of features because they split data based on feature values, not distance or gradient descent.

2. Random Forest**:
   - Based on decision trees, so it inherits the same insensitivity to feature scaling.

3. Bagging (Bootstrap Aggregating)**:
   - Often used with decision trees (e.g., Random Forest), which do not require scaling.

4. Extra Trees (Extremely Randomized Trees)**:
   - Like Random Forests, it doesn’t require scaling due to its tree-based structure.

5. Naive Bayes**:
   - Based on probability calculations, not distance or gradient descent, so it’s insensitive to the scale of features.

6. Rule-based Algorithms (e.g., RIPPER, PART)**:
   - These algorithms create rules based on feature values, not distances, so scaling is not necessary.

7. Ordinary Least Squares Linear Regression (OLS)**:
   - Coefficients can adjust for different feature scales, so scaling is not strictly required unless optimization or regularization is involved.

8. AdaBoost**:
   - Usually used with decision trees, which are insensitive to feature scales.

---

*************  How we deal with outliers in our dataset?

1. we can remove data having outliers completely

2. But if we don't want to lose our data then we can apply various techniques
### **Special Cases**:
- **Ensemble Methods**: Depending on the base model, feature scaling may or may not be necessary. For example, **Bagging** or **Boosting** with trees doesn't require scaling, but if used with **SVMs or KNN**, scaling is required.
  
---

### **Summary**:
- **Required**: Algorithms that use distance metrics (like SVM, KNN) or gradient-based optimization (like neural networks, logistic regression) benefit from feature scaling.
- **Not Required**: Algorithms based on tree structures (like decision trees, random forests) or those based on rules or probabilities (like Naive Bayes).

It's a good rule of thumb to always check whether the algorithm you're using depends on distance or gradient descent, as those are the key factors that make scaling necessary.

## decision tree regressor

# Importing necessary libraries
from sklearn.tree import DecisionTreeRegressor

# Creating and initializing the Decision Tree Regressor
regressor = DecisionTreeRegressor(max_depth=5, random_state=42)

# Training the regressor on the training data
regressor.fit(X_train, y_train)

# Making predictions on the test data
y_pred = regressor.predict(X_test)

decision tree regression model does not works well on 2d data set but for higher dimention data sets will many features it works well
decision tree has low bias and high variance

## Random Forest

s1- pick random K data points  from the training set

s2- Build the decision tree associated with K data points

s3- Choose the number of trees you want to build and repeat step 1 and 2.

we either take mean or median of the output obtained by each decision tree

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error

# Creating and training the Random Forest Regressor
regressor = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)
regressor.fit(X_train, y_train)

# Making predictions on the test set
y_pred = regressor.predict(X_test)

# Print the shape of the prediction array to confirm it is 1D
print(f"Shape of y_pred: {y_pred.shape}")  # Should print (20,)

# Evaluating the model
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")

-------------------****************------------------

# Logistic Regression

from sklearn.linear_model import LogisticRegression

classifier = LogisticRegression(random_state=0)
classifier.fit(X_train, y_train)

classfier.predict(sc.transform([[30,87000]]))            // to apply feature scaling on test data


// to print test result and actual result simultaneously
y_pred=classifier.predict(X_test)
np.set_printoptions(precision=2)

print(np.concantenate((y_pred.reshape(len(y_pred),1),y_test.reshape(len(y_test),1)),1))


----------------------------****************************---------------------------------------


What is Regularization?

Regularization is a technique used in machine learning to prevent overfitting by adding a penalty to the model's complexity. It helps improve the model's generalization to unseen data by discouraging it from fitting the noise in the training dataset.

Types of Regularization
L1 Regularization (Lasso Regression)

Adds the absolute value of the coefficients as a penalty term to the cost function.

Encourages sparsity, meaning some coefficients are driven to zero, effectively performing feature selection.

L2 Regularization (Ridge Regression)

Adds the squared value of the coefficients as a penalty term to the cost function.
​
 
Prevents coefficients from becoming too large, leading to a more stable model.
Elastic Net Regularization

Combines both L1 and L2 regularization.
 
Balances the benefits of both L1 and L2 regularization.


----------------------------****************************---------------------------------------

What is linear classification model and non linear classification model?

A linear classification model makes predictions based on a linear decision boundary. This boundary is a straight line (or hyperplane in higher dimensions) that separates different classes.

Example: Logistic Regression , SVM with linear kernel


Non-Linear Classification Model:
Concept:

A non-linear classification model can handle more complex relationships between features and classes. It does not rely on a straight line or hyperplane to make predictions.
Example: Decision Trees ,Random Forest,Neural networks , SVM with non linear kernel , K-Nearest Neighbour


What is support vector machine?

- SVM is like drawing the best possible line or boundary between two groups of data points.

- It finds this boundary by making sure the line has the largest margin between the two groups.
- The closest points to the boundary (support vectors) are the most important in deciding where to place this line.
- Sometimes, the data isn’t easily separable by a straight line. In such cases, SVM can use a technique called kernel trick. This technique transforms the data into a higher-dimensional space where it might be easier to find a separating line.

What is Kernel trick?

--When data isn’t easily separable by a straight line (in 2D) or a flat plane (in higher dimensions), the kernel trick helps transform the data into a higher-dimensional space where it can be more easily separated by a linear boundary.

--This mapping happens without actually calculating the new higher-dimensional points, which saves computation.(How?)

how the kernel trick works without directly calculating the higher-dimensional points?

-The Key Idea: Inner Product in the Higher Dimension
-The core of the kernel trick is based on inner products (or dot products) of data points.

-When you map data to a higher-dimensional space, instead of explicitly transforming each data point into a higher dimension, the kernel trick computes the dot product of the transformed data points directly in the original space. This means:

-You don’t need to know the exact coordinates of the transformed points.
-You only need a function (the kernel function) that can give you the same result as if you had transformed the points.



*******Code - for SVM

# import libraries

from sklearn.svm import SVC

# Create and train the SVM model with a linear kernel
svm_classifier = SVC(kernel='linear', random_state=0)
svm_classifier.fit(X_train, y_train)


# Create and train the SVM model with a Non-linear kernel
svm_classifier = SVC(kernel='rbf', random_state=0)
svm_classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred = svm_classifier.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)


-- To remove duplicate rows from your dataset before training the model, you can use the drop_duplicates() method in pandas.

*****  What is  support vector regressor?
# import libraries

from sklearn.svm import SVR

X=dataset.iloc[:,:-1].values
y=dataset.iloc[:,-1].values

# in svr we don't need to split training set and test set
scaled_X=StandardScaler()
scaled_y=StandardScaler()

X_train = scaled_X.fit_transform(X)

y_train = scaled_y.fit_transform(y.reshape(len(y),1))



# Create and train the SVM model with a linear kernel
svm_regressor = SVR(kernel='rbf', random_state=0)
svm_regressor.fit(X, y)


# Make predictions on the test set
y_pred = svm_regressor.predict(scaled_X.transform([[6.5]]).reshape(-1,1))

# Reverse the scaling to get predictions back in the original scale
y_pred_original = scaled_y.inverse_transform(y_pred)

print()

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)


------------********* Code for Naïve Bayes

from sklearn.naive_bayes import GaussianNB
clf = GaussianNB()
clf.fit(X_train, y_train)



# Apply feature scaling on the training and test sets
# Separate features and target , sometimes we have to apply feature scaling on dependent variables too

X=dataset.iloc[:,:-1].values
y=dataset.iloc[:,-1].values

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1)

scaled_X=StandardScaler()
scaled_y=StandardScaler()

X_train = scaled_X.fit_transform(X_train)
X_test = scaled_X.transform(X_test)

y_train = scaled_y.fit_transform(y_train)

y_test = scaled_y.transform(y_test)


** We can compare various regression model , compare their accuracy by calculating R-Square error for each one and find the best model for our data

** What is Cumulative accuracy profile (Cap curve): The Cumulative Accuracy Profile (CAP) is a visual tool used to evaluate the effectiveness of a classification model. It shows how well the model is at capturing positive cases (e.g., buyers) as you contact customers, helping you compare your model's performance against a perfect model or a random selection.

****** K-means clustering algorithm

In **K-means clustering**, **WCSS** stands for **Within-Cluster Sum of Squares**. It is a metric used to measure the compactness of the clusters formed by the K-means algorithm. Specifically, WCSS calculates the sum of the squared distances between each data point and the centroid (center) of its assigned cluster.

### What WCSS Does:
- **Lower WCSS** values indicate that the data points are closer to the centroids of their respective clusters, meaning that the clusters are well-formed and compact.
- **Higher WCSS** values suggest that the data points are spread out and the clusters are not as tight.

The total WCSS is then the sum of WCSS values for all clusters.

### Role in K-means:
- **Minimizing WCSS** is the main objective of the K-means algorithm. It tries to form clusters that minimize the WCSS by adjusting the positions of the centroids in each iteration.
- **The "Elbow Method"**: WCSS is also used in determining the optimal number of clusters \( K \). By plotting WCSS for different values of \( K \), you often observe a sharp bend or "elbow." The value of \( K \) at this point typically represents the optimal number of clusters because adding more clusters beyond this point doesn’t significantly reduce WCSS.

In short, WCSS helps evaluate how well K-means is clustering the data and assists in selecting the appropriate number of clusters.

*****code for implementation of K-means algorithm

--- Elbow method to find optimal number of K(number of clusters)

from sklearn.cluster import KMeans

X=dataset.iloc[:,:-1].values
y=dataset.iloc[:,-1].values

wcss=[]
for i in range(1,11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

plt.plot(range(1,11),wcss)
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()








