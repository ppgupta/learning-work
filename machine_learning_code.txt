# Importing the necessary libraries
import pandas as pd 
import numpy as np 
from sklearn.preprocessing import OneHotEncoder ,LabelEncoder

from sklearn.compose import ColumnTransformer

# Load the dataset
dataset=pd.read_csv('titanic.csv')

# Identify the categorical data

categorical_feature = ['Sex', 'Embarked', 'Pclass']

# Implement an instance of the ColumnTransformer class
ct=ColumnTransformer(transformers=[('encoder',OneHotEncoder(),categorical_feature)],remainder='passthrough')

# Apply the fit_transform method on the instance of ColumnTransformer
X=ct.fit_transform(dataset)


# Convert the output into a NumPy array
X=np.array(X)

# Use LabelEncoder to encode binary categorical data
le = LabelEncoder()
y = le.fit_transform(dataset['Survived'])

# Print the updated matrix of features and the dependent variable vector
print("Updated matrix of features: \n", X)
print("Updated dependent variable vector: \n", y)


***
# Import necessary libraries
import pandas as pd
import numpy as np

from sklearn.preprocessing import OneHotEncoder,LabelEncoder
from sklearn.preprocessing import StandardScaler

from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer

from sklearn.impute import SimpleImputer
# Load the Iris dataset

dataset=pd.read_csv('iris.csv')
# Separate features and target
X=dataset.iloc[:,:-1].values
y=dataset.iloc[:,-1].values

# Split the dataset into an 80-20 training-test set

X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=1)

# Apply feature scaling on the training and test sets
scaler=StandardScaler()
X_train=scaler.fit_transform(X_train)
X_test=scaler.fit_transform(X_test)

# Print the scaled training and test sets
print("Scaled Training Set:")
print(X_train)
print("\nScaled Test Set:")
print(X_test)

-- we should not apply feature scaling standardisation to encoded data because doing so will make actual interpretation of encoded data

**Feature scaling
# Import necessary libraries
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
# Load the Wine Quality Red dataset

dataset=pd.read_csv('winequality-red.csv', delimiter=';')
# Separate features and target
X=dataset.iloc[:,:-1].values
y=dataset.iloc[:,-1].values

# X = df.drop('quality', axis=1)
# y = df['quality']
# Split the dataset into an 80-20 training-test set
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)


# Create an instance of the StandardScaler class
scaler=StandardScaler()



# Fit the StandardScaler on the features from the training set and transform it

X_train=scaler.fit_transform(X_train)
# Apply the transform to the test set
X_test=scaler.transform(X_test)


# Print the scaled training and test datasets
print(X_train)
print(X_test)
